{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: autotrain <command> [<args>] llm [-h] [--train] [--deploy]\n",
      "                                        [--inference] [--backend BACKEND]\n",
      "                                        [--model MODEL]\n",
      "                                        [--project-name PROJECT_NAME]\n",
      "                                        [--data-path DATA_PATH]\n",
      "                                        [--train-split TRAIN_SPLIT]\n",
      "                                        [--valid-split VALID_SPLIT]\n",
      "                                        [--add-eos-token]\n",
      "                                        [--model-max-length MODEL_MAX_LENGTH]\n",
      "                                        [--padding PADDING]\n",
      "                                        [--trainer TRAINER]\n",
      "                                        [--use-flash-attention-2] [--log LOG]\n",
      "                                        [--disable-gradient-checkpointing]\n",
      "                                        [--logging-steps LOGGING_STEPS]\n",
      "                                        [--eval-strategy EVAL_STRATEGY]\n",
      "                                        [--save-total-limit SAVE_TOTAL_LIMIT]\n",
      "                                        [--auto-find-batch-size]\n",
      "                                        [--mixed-precision MIXED_PRECISION]\n",
      "                                        [--lr LR] [--epochs EPOCHS]\n",
      "                                        [--batch-size BATCH_SIZE]\n",
      "                                        [--warmup-ratio WARMUP_RATIO]\n",
      "                                        [--gradient-accumulation GRADIENT_ACCUMULATION]\n",
      "                                        [--optimizer OPTIMIZER]\n",
      "                                        [--scheduler SCHEDULER]\n",
      "                                        [--weight-decay WEIGHT_DECAY]\n",
      "                                        [--max-grad-norm MAX_GRAD_NORM]\n",
      "                                        [--seed SEED]\n",
      "                                        [--chat-template CHAT_TEMPLATE]\n",
      "                                        [--quantization QUANTIZATION]\n",
      "                                        [--target-modules TARGET_MODULES]\n",
      "                                        [--merge-adapter] [--peft]\n",
      "                                        [--lora-r LORA_R]\n",
      "                                        [--lora-alpha LORA_ALPHA]\n",
      "                                        [--lora-dropout LORA_DROPOUT]\n",
      "                                        [--model-ref MODEL_REF]\n",
      "                                        [--dpo-beta DPO_BETA]\n",
      "                                        [--max-prompt-length MAX_PROMPT_LENGTH]\n",
      "                                        [--max-completion-length MAX_COMPLETION_LENGTH]\n",
      "                                        [--prompt-text-column PROMPT_TEXT_COLUMN]\n",
      "                                        [--text-column TEXT_COLUMN]\n",
      "                                        [--rejected-text-column REJECTED_TEXT_COLUMN]\n",
      "                                        [--push-to-hub] [--username USERNAME]\n",
      "                                        [--token TOKEN] [--unsloth]\n",
      "                                        [--block_size BLOCK_SIZE]\n",
      "\n",
      "âœ¨ Run AutoTrain LLM\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --train               Command to train the model\n",
      "  --deploy              Command to deploy the model (limited availability)\n",
      "  --inference           Command to run inference (limited availability)\n",
      "  --backend BACKEND     Backend\n",
      "  --model MODEL, --model MODEL, --model MODEL\n",
      "                        Model name\n",
      "  --project-name PROJECT_NAME, --project_name PROJECT_NAME, --project-name PROJECT_NAME\n",
      "                        Output directory\n",
      "  --data-path DATA_PATH, --data_path DATA_PATH, --data-path DATA_PATH\n",
      "                        Data path\n",
      "  --train-split TRAIN_SPLIT, --train_split TRAIN_SPLIT, --train-split TRAIN_SPLIT\n",
      "                        Train data config\n",
      "  --valid-split VALID_SPLIT, --valid_split VALID_SPLIT, --valid-split VALID_SPLIT\n",
      "                        Validation data config\n",
      "  --add-eos-token, --add_eos_token, --add-eos-token\n",
      "                        Add EOS token\n",
      "  --model-max-length MODEL_MAX_LENGTH, --model_max_length MODEL_MAX_LENGTH, --model-max-length MODEL_MAX_LENGTH\n",
      "                        Model max length\n",
      "  --padding PADDING, --padding PADDING, --padding PADDING\n",
      "                        Padding side\n",
      "  --trainer TRAINER, --trainer TRAINER, --trainer TRAINER\n",
      "                        Trainer type\n",
      "  --use-flash-attention-2, --use_flash_attention_2, --use-flash-attention-2\n",
      "                        Use flash attention 2\n",
      "  --log LOG, --log LOG, --log LOG\n",
      "                        Logging using experiment tracking\n",
      "  --disable-gradient-checkpointing, --disable_gradient_checkpointing, --disable-gradient-checkpointing\n",
      "                        Gradient checkpointing\n",
      "  --logging-steps LOGGING_STEPS, --logging_steps LOGGING_STEPS, --logging-steps LOGGING_STEPS\n",
      "                        Logging steps\n",
      "  --eval-strategy EVAL_STRATEGY, --eval_strategy EVAL_STRATEGY, --eval-strategy EVAL_STRATEGY\n",
      "                        Evaluation strategy\n",
      "  --save-total-limit SAVE_TOTAL_LIMIT, --save_total_limit SAVE_TOTAL_LIMIT, --save-total-limit SAVE_TOTAL_LIMIT\n",
      "                        Save total limit\n",
      "  --auto-find-batch-size, --auto_find_batch_size, --auto-find-batch-size\n",
      "                        Auto find batch size\n",
      "  --mixed-precision MIXED_PRECISION, --mixed_precision MIXED_PRECISION, --mixed-precision MIXED_PRECISION\n",
      "                        fp16, bf16, or None\n",
      "  --lr LR, --lr LR, --lr LR\n",
      "                        Learning rate\n",
      "  --epochs EPOCHS, --epochs EPOCHS, --epochs EPOCHS\n",
      "                        Number of training epochs\n",
      "  --batch-size BATCH_SIZE, --batch_size BATCH_SIZE, --batch-size BATCH_SIZE\n",
      "                        Training batch size\n",
      "  --warmup-ratio WARMUP_RATIO, --warmup_ratio WARMUP_RATIO, --warmup-ratio WARMUP_RATIO\n",
      "                        Warmup proportion\n",
      "  --gradient-accumulation GRADIENT_ACCUMULATION, --gradient_accumulation GRADIENT_ACCUMULATION, --gradient-accumulation GRADIENT_ACCUMULATION\n",
      "                        Gradient accumulation steps\n",
      "  --optimizer OPTIMIZER, --optimizer OPTIMIZER, --optimizer OPTIMIZER\n",
      "                        Optimizer\n",
      "  --scheduler SCHEDULER, --scheduler SCHEDULER, --scheduler SCHEDULER\n",
      "                        Scheduler\n",
      "  --weight-decay WEIGHT_DECAY, --weight_decay WEIGHT_DECAY, --weight-decay WEIGHT_DECAY\n",
      "                        Weight decay\n",
      "  --max-grad-norm MAX_GRAD_NORM, --max_grad_norm MAX_GRAD_NORM, --max-grad-norm MAX_GRAD_NORM\n",
      "                        Max gradient norm\n",
      "  --seed SEED, --seed SEED, --seed SEED\n",
      "                        Seed\n",
      "  --chat-template CHAT_TEMPLATE, --chat_template CHAT_TEMPLATE, --chat-template CHAT_TEMPLATE\n",
      "                        Chat template, one of: None, zephyr, chatml or\n",
      "                        tokenizer\n",
      "  --quantization QUANTIZATION, --quantization QUANTIZATION, --quantization QUANTIZATION\n",
      "                        int4, int8, or None\n",
      "  --target-modules TARGET_MODULES, --target_modules TARGET_MODULES, --target-modules TARGET_MODULES\n",
      "                        Target modules\n",
      "  --merge-adapter, --merge_adapter, --merge-adapter\n",
      "                        Merge adapter\n",
      "  --peft, --peft, --peft\n",
      "                        Use PEFT\n",
      "  --lora-r LORA_R, --lora_r LORA_R, --lora-r LORA_R\n",
      "                        Lora r\n",
      "  --lora-alpha LORA_ALPHA, --lora_alpha LORA_ALPHA, --lora-alpha LORA_ALPHA\n",
      "                        Lora alpha\n",
      "  --lora-dropout LORA_DROPOUT, --lora_dropout LORA_DROPOUT, --lora-dropout LORA_DROPOUT\n",
      "                        Lora dropout\n",
      "  --model-ref MODEL_REF, --model_ref MODEL_REF, --model-ref MODEL_REF\n",
      "                        Reference, for DPO trainer\n",
      "  --dpo-beta DPO_BETA, --dpo_beta DPO_BETA, --dpo-beta DPO_BETA\n",
      "                        Beta for DPO trainer\n",
      "  --max-prompt-length MAX_PROMPT_LENGTH, --max_prompt_length MAX_PROMPT_LENGTH, --max-prompt-length MAX_PROMPT_LENGTH\n",
      "                        Prompt length\n",
      "  --max-completion-length MAX_COMPLETION_LENGTH, --max_completion_length MAX_COMPLETION_LENGTH, --max-completion-length MAX_COMPLETION_LENGTH\n",
      "                        Completion length\n",
      "  --prompt-text-column PROMPT_TEXT_COLUMN, --prompt_text_column PROMPT_TEXT_COLUMN, --prompt-text-column PROMPT_TEXT_COLUMN\n",
      "                        Prompt text column\n",
      "  --text-column TEXT_COLUMN, --text_column TEXT_COLUMN, --text-column TEXT_COLUMN\n",
      "                        Text column\n",
      "  --rejected-text-column REJECTED_TEXT_COLUMN, --rejected_text_column REJECTED_TEXT_COLUMN, --rejected-text-column REJECTED_TEXT_COLUMN\n",
      "                        Rejected text column\n",
      "  --push-to-hub, --push_to_hub, --push-to-hub\n",
      "                        Push to hub\n",
      "  --username USERNAME, --username USERNAME, --username USERNAME\n",
      "                        Hugging Face Username\n",
      "  --token TOKEN, --token TOKEN, --token TOKEN\n",
      "                        Huggingface token\n",
      "  --unsloth, --unsloth, --unsloth\n",
      "                        Use unsloth\n",
      "  --block_size BLOCK_SIZE, --block-size BLOCK_SIZE\n",
      "                        Block size\n"
     ]
    }
   ],
   "source": [
    "!autotrain llm -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-09-14 10:36:05\u001b[0m | \u001b[36mautotrain.cli.run_llm\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mRunning LLM\u001b[0m\n",
      "\u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[32m2024-09-14 10:36:05\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m180\u001b[0m - \u001b[33m\u001b[1mParameters supplied but not used: backend, inference, func, deploy, version, train, config\u001b[0m\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆ| 15/15 [00:00<00:00, 9172.56 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆ| 15/15 [00:00<00:00, 10699.76 examples/s\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-09-14 10:36:05\u001b[0m | \u001b[36mautotrain.backends.local\u001b[0m:\u001b[36mcreate\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mStarting local training...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-09-14 10:36:05\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m489\u001b[0m - \u001b[1m['accelerate', 'launch', '--num_machines', '1', '--num_processes', '1', '--mixed_precision', 'fp16', '-m', 'autotrain.trainers.clm', '--training_config', 'cs-trial/training_params.json']\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-09-14 10:36:05\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m490\u001b[0m - \u001b[1m{'model': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0', 'project_name': 'cs-trial', 'data_path': 'cs-trial/autotrain-data', 'train_split': 'train', 'valid_split': None, 'add_eos_token': True, 'block_size': 1024, 'model_max_length': 8192, 'padding': 'right', 'trainer': 'sft', 'use_flash_attention_2': False, 'log': 'none', 'disable_gradient_checkpointing': False, 'logging_steps': -1, 'eval_strategy': 'epoch', 'save_total_limit': 1, 'auto_find_batch_size': True, 'mixed_precision': 'fp16', 'lr': 0.0002, 'epochs': 3, 'batch_size': 2, 'warmup_ratio': 0.1, 'gradient_accumulation': 4, 'optimizer': 'adamw_torch', 'scheduler': 'linear', 'weight_decay': 0.0, 'max_grad_norm': 1.0, 'seed': 42, 'chat_template': 'tokenizer', 'quantization': 'int4', 'target_modules': 'all-linear', 'merge_adapter': False, 'peft': True, 'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.0, 'model_ref': None, 'dpo_beta': 0.1, 'max_prompt_length': 128, 'max_completion_length': None, 'prompt_text_column': 'autotrain_prompt', 'text_column': 'autotrain_text', 'rejected_text_column': 'autotrain_rejected_text', 'push_to_hub': False, 'username': None, 'token': '*****', 'unsloth': False}\u001b[0m\n",
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-09-14 10:36:11\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_sft\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mStarting SFT training...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-09-14 10:36:11\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m339\u001b[0m - \u001b[1mloading dataset from disk\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-09-14 10:36:11\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m398\u001b[0m - \u001b[1mTrain data: Dataset({\n",
      "    features: ['autotrain_text'],\n",
      "    num_rows: 15\n",
      "})\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-09-14 10:36:11\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1mValid data: None\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-09-14 10:36:11\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_data_with_chat_template\u001b[0m:\u001b[36m450\u001b[0m - \u001b[1mApplying chat template\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-09-14 10:36:11\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_data_with_chat_template\u001b[0m:\u001b[36m451\u001b[0m - \u001b[1mFor ORPO/DPO, `prompt` will be extracted from chosen messages\u001b[0m\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 470.41 examples/s]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-09-14 10:36:11\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_logging_steps\u001b[0m:\u001b[36m471\u001b[0m - \u001b[1mconfiguring logging steps\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-09-14 10:36:11\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_logging_steps\u001b[0m:\u001b[36m484\u001b[0m - \u001b[1mLogging steps: 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-09-14 10:36:11\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_training_args\u001b[0m:\u001b[36m489\u001b[0m - \u001b[1mconfiguring training args\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-09-14 10:36:11\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_block_size\u001b[0m:\u001b[36m552\u001b[0m - \u001b[1mUsing block size 1024\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-09-14 10:36:11\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m587\u001b[0m - \u001b[1mCan use unsloth: False\u001b[0m\n",
      "\u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[32m2024-09-14 10:36:11\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m629\u001b[0m - \u001b[33m\u001b[1mUnsloth not available, continuing without it...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-09-14 10:36:11\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m631\u001b[0m - \u001b[1mloading model config...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-09-14 10:36:11\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m639\u001b[0m - \u001b[1mloading model...\u001b[0m\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-09-14 10:36:15\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m670\u001b[0m - \u001b[1mmodel dtype: torch.float16\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-09-14 10:36:15\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_sft\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mcreating trainer\u001b[0m\n",
      "Generating train split: 7 examples [00:00, 376.91 examples/s]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-09-14 10:36:16\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_train_begin\u001b[0m:\u001b[36m230\u001b[0m - \u001b[1mStarting to train...\u001b[0m\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]/home/ssegpu/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 1/3 [00:01<00:02,  1.23s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-09-14 10:36:18\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.0997, 'grad_norm': 2.953204393386841, 'learning_rate': 0.0002, 'epoch': 1.0}\u001b[0m\n",
      "{'loss': 1.0997, 'grad_norm': 2.953204393386841, 'learning_rate': 0.0002, 'epoch': 1.0}\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 2/3 [00:02<00:00,  1.02it/s]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-09-14 10:36:18\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.1133, 'grad_norm': 2.870089054107666, 'learning_rate': 0.0001, 'epoch': 2.0}\u001b[0m\n",
      "{'loss': 1.1133, 'grad_norm': 2.870089054107666, 'learning_rate': 0.0001, 'epoch': 2.0}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.11it/s]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-09-14 10:36:19\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 0.9382, 'grad_norm': 0.6681914925575256, 'learning_rate': 0.0, 'epoch': 3.0}\u001b[0m\n",
      "{'loss': 0.9382, 'grad_norm': 0.6681914925575256, 'learning_rate': 0.0, 'epoch': 3.0}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.11it/s]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-09-14 10:36:19\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'train_runtime': 2.8504, 'train_samples_per_second': 7.367, 'train_steps_per_second': 1.052, 'train_loss': 1.0504190723101299, 'epoch': 3.0}\u001b[0m\n",
      "{'train_runtime': 2.8504, 'train_samples_per_second': 7.367, 'train_steps_per_second': 1.052, 'train_loss': 1.0504190723101299, 'epoch': 3.0}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.05it/s]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-09-14 10:36:19\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mpost_training_steps\u001b[0m:\u001b[36m288\u001b[0m - \u001b[1mFinished training, saving model...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-09-14 10:36:21\u001b[0m | \u001b[36mautotrain.cli.run_llm\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mJob ID: 2466435\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "autotrain llm \\\n",
    "--train \\\n",
    "--project-name cs-trial \\\n",
    "--model TinyLlama/TinyLlama-1.1B-Chat-v1.0 \\\n",
    "--data-path /home/ssegpu/TypeEvalPy/TypeEvalPy/extras/fine_tuning/data_cs \\\n",
    "--train-split train \\\n",
    "--chat_template tokenizer \\\n",
    "--peft \\\n",
    "--quantization int4 \\\n",
    "--lr 2e-4 \\\n",
    "--trainer sft \\\n",
    "--auto-find-batch-size \\\n",
    "--epochs 3 \\\n",
    "--model_max_length 8192 \\\n",
    "--mixed_precision fp16 \\\n",
    "--lora_dropout 0 \\\n",
    "--token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "# Array of models\n",
    "models=(\n",
    "  \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "  \"google/codegemma-2b\"\n",
    ")\n",
    "# \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "# \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "# \"codellama/CodeLlama-34b-Python-hf\"\n",
    "# \"codellama/CodeLlama-13b-Python-hf\"\n",
    "# \"codellama/CodeLlama-70b-Python-hf\"\n",
    "\n",
    "\n",
    "# Loop through each model\n",
    "for model in \"${models[@]}\"; do\n",
    "  # Extract model name without slashes or special characters for project name\n",
    "  project_name=$(echo \"$model\" | sed 's/[\\/.]/-/g')-sl-v1\n",
    "\n",
    "  # Run the autotrain command with dynamic project name and model\n",
    "  autotrain llm \\\n",
    "  --train \\\n",
    "  --project-name \"$project_name\" \\\n",
    "  --model \"$model\" \\\n",
    "  --data-path /home/ssegpu/fine-tuning-apsv/single-label-data \\\n",
    "  --train-split train \\\n",
    "  --valid-split validation \\\n",
    "  --chat_template tokenizer \\\n",
    "  --use-peft \\\n",
    "  --quantization int4 \\\n",
    "  --lr 2e-4 \\\n",
    "  --trainer sft \\\n",
    "  --auto_find_batch_size \\\n",
    "  --mixed_precision fp16 \\\n",
    "  --model_max_length 8192 \\\n",
    "  --lora_dropout 0 \\\n",
    "  --epochs 3 \\\n",
    "  --token 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\n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v3-smaller batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!autotrain llm \\\n",
    "--train \\\n",
    "--project-name codellama-70B-python-finetuned-apsv \\\n",
    "--model meta-llama/CodeLlama-70b-Python-hf \\\n",
    "--data-path /home/ssegpu/fine-tuning-apsv/data \\\n",
    "--train-split train \\\n",
    "--valid-split validation \\\n",
    "--chat_template tokenizer \\\n",
    "--use-peft \\\n",
    "--quantization int4 \\\n",
    "--lr 2e-5 \\\n",
    "--trainer sft \\\n",
    "--train-batch-size 6 \\\n",
    "--epochs 3 \\\n",
    "--model_max_length 8192 \\\n",
    "--mixed_precision fp16 \\\n",
    "--lora_dropout 0 \\\n",
    "--token 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\n",
    "# --push-to-hub \\\n",
    "# --username ashwinprasadme \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!autotrain llm \\\n",
    "--train \\\n",
    "--project-name llama3-8b-finetuned-apsv-v4 \\\n",
    "--model meta-llama/Meta-Llama-3-8B \\\n",
    "--data-path /home/ssegpu/fine-tuning-apsv/data \\\n",
    "--train-split train \\\n",
    "--valid-split validation \\\n",
    "--chat_template tokenizer \\\n",
    "--use-peft \\\n",
    "--quantization int4 \\\n",
    "--lr 2e-4 \\\n",
    "--train-batch-size 12 \\\n",
    "--epochs 3 \\\n",
    "--trainer sft \\\n",
    "--token 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\n",
    "# --push-to-hub \\\n",
    "# --username ashwinprasadme \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V5\n",
    "\n",
    "Based on \n",
    "- https://huggingface.co/docs/autotrain/llm_finetuning_params\n",
    "- https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms\n",
    "- https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html\n",
    "\n",
    "---\n",
    "/home/ssegpu/finetune-llms/autotrain-docker/autotrain-advanced/src/autotrain/cli/utils.py\n",
    "\n",
    "- --auto_find_batch_size\n",
    "  \n",
    "    - \"help\": \"Automatically determine the optimal batch size based on system capabilities to maximize efficiency.\",\n",
    "\n",
    "- --lora_r\n",
    "\n",
    "    - \"help\": \"Set the 'r' parameter for Low-Rank Adaptation (LoRA). Default is 16.\",\n",
    "\n",
    "- --lora_alpha\n",
    "    - \"help\": \"Specify the 'alpha' parameter for LoRA. Default is 32.\",\n",
    "\n",
    "- --lora_dropout\n",
    "    - \"help\": \"Set the dropout rate within the LoRA layers to help prevent overfitting during adaptation. Default is 0.05.\",\n",
    "\n",
    "- --epochs\n",
    "    - \"help\": \"Number of training epochs\"\n",
    "\n",
    "- --mixed_precision fp16 \n",
    "\n",
    "- --scheduler\n",
    "\n",
    "    - \"help\": \"Select the learning rate scheduler to adjust the learning rate based on the number of epochs. 'linear' decreases the learning rate linearly from the initial lr set. Default is 'linear'. Try 'cosine' for a cosine annealing schedule.\",\n",
    "\n",
    "- --weight_decay\n",
    "\n",
    "    - \"help\": \"Define the weight decay rate for regularization, which helps prevent overfitting by penalizing larger weights. Default is 0.0\",\n",
    "\n",
    "    Generally, a larger r can lead to more overfitting because it determines the number of trainable parameters. If a model suffers from overfitting, decreasing r or increasing the dataset size are the first candidates to explore. Moreover, you could try to increase the weight decay rate in AdamW or SGD optimizers, and you can consider increasing the dropout value for LoRA layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!autotrain llm \\\n",
    "--train \\\n",
    "--project-name llama3-8b-finetuned-apsv-v5 \\\n",
    "--model meta-llama/Meta-Llama-3-8B \\\n",
    "--data-path /home/ssegpu/fine-tuning-apsv/data \\\n",
    "--train-split train \\\n",
    "--valid-split validation \\\n",
    "--chat_template tokenizer \\\n",
    "--use-peft \\\n",
    "--quantization int4 \\\n",
    "--lr 3e-4 \\\n",
    "--trainer sft \\\n",
    "--token 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 \\\n",
    "--train-batch-size 12 \\\n",
    "--lora_r 32 \\\n",
    "--lora_alpha 64 \\\n",
    "--epochs 3 \\\n",
    "--mixed_precision fp16 \\\n",
    "--scheduler cosine \\\n",
    "--weight_decay 0.02 \\\n",
    "--model_max_length 8192\n",
    "# --train-batch-size 12 \\\n",
    "# --push-to-hub \\\n",
    "# --username ashwinprasadme \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- --gradient-accumulation\n",
    "    - \"help\": \"Gradient accumulation steps\",\n",
    "\n",
    "    https://discuss.huggingface.co/t/batch-size-vs-gradient-accumulation/5260/3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
