import os
import time
import torch
import transformers
import json
import csv
import re
from ast import literal_eval
import pandas as pd
from pathlib import Path
from tqdm.auto import tqdm
from sklearn.metrics import (
    classification_report,
    accuracy_score,
    hamming_loss,
    jaccard_score,
)
from sklearn.preprocessing import MultiLabelBinarizer
from vllm import LLM, SamplingParams
from vllm.lora.request import LoRARequest

import vllm_helpers
import gc
import torch


# Load model, tokenizer and create pipeline
def load_model_and_configurations(HF_TOKEN, model_name, use_quantized_model=True):
    temperature = 0.001

    bnb_config = transformers.BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
    )

    model = transformers.AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",
        quantization_config=bnb_config if use_quantized_model else None,
        token=HF_TOKEN,
    )

    tokenizer = transformers.AutoTokenizer.from_pretrained(
        model_name,
        token=HF_TOKEN,
    )

    pipe = transformers.pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        pad_token_id=tokenizer.eos_token_id,
        temperature=temperature,
    )

    return pipe


def fix_syntax_issues(text):
    try:
        parsed_text = literal_eval(text)
        if isinstance(parsed_text, list):
            return "\n".join(parsed_text)
        else:
            return text
    except (ValueError, SyntaxError):
        fixed_text = text.replace("\\n", "\n").replace("\\'", "'")
        return fixed_text


def create_prompt(row, prompt_template, use_system_prompt=True):
    try:
        code_cell = fix_syntax_issues(row["text"])
    except (ValueError, SyntaxError) as e:
        print(f"Error processing row {row.name}: {e}")
        with open("problematic_rows.log", "a") as log_file:
            log_file.write(f"Row {row.name}: {e}\n")
    prompt = json.loads(json.dumps(prompt_template))
    if use_system_prompt:
        prompt[1]["content"] = prompt[1]["content"].format(code=code_cell)
    else:
        prompt[0]["content"] = prompt[0]["content"].format(code=code_cell)

    # remove last item only if role is assistant
    if prompt[-1]["role"] == "assistant":
        prompt.pop()

    return prompt


# Function to parse valid tokens from output generated by llm
def parse_response(input_dict):

    input_string = input_dict.get("content", "")

    valid_items = [
        "helper_functions",
        "load_data",
        "data_exploration",
        "data_preprocessing",
        "evaluation",
        "modelling",
        "prediction",
        "result_visualization",
        "save_results",
        "comment_only",
    ]

    def create_variations(item):
        base = item.replace("_", " ")
        return {
            base: item,
            base.replace(" ", "_"): item,
            base.replace(" ", "-"): item,
            base.replace("_", " "): item,
            base.replace("-", " "): item,
            base.replace("_", "-"): item,
            base.replace("-", "_"): item,
            base.replace("\\_", "_"): item,
        }

    variation_map = {}
    for item in valid_items:
        variation_map.update(create_variations(item))

    clean_string = re.sub(r"[^\x00-\x7F]+", " ", input_string)

    clean_string = re.sub(r"[^\w\s,\\]+", " ", clean_string)
    clean_string = re.sub(r"\\_", "_", clean_string)

    matches = []
    for variation, canonical in variation_map.items():
        if re.search(r"\b" + re.escape(variation) + r"\b", clean_string, re.IGNORECASE):
            matches.append(canonical)

    valid_tokens = list(set(matches))

    return valid_tokens


# Function to write the response from llm and parsed_response to a csv file with ID of the record for further analysis
def response_to_csv(id, output, prompt_time, results_dump_file):
    os.makedirs(os.path.dirname(results_dump_file), exist_ok=True)

    columns = ["ID", "response_from_model", "parsed_response", "prompt_time"]

    parsed_labels = parse_response(output)

    data = [[id, output, parsed_labels, prompt_time]]

    with open(results_dump_file, "a", newline="") as file:
        writer = csv.writer(file)

        if file.tell() == 0:
            writer.writerow(columns)

        writer.writerows(data)


# Model Evaluation on test dataset
def load_dataset(DATASET_PATH):
    with open(DATASET_PATH, "r") as csv_file:
        dataset_rows = list(csv.DictReader(csv_file))
    return dataset_rows


def model_evaluation(
    model_name,
    prompt_template,
    dataset_rows,
    pipe,
    RESULTS_PATH,
    use_system_prompt=False,
):
    results_dst = Path(RESULTS_PATH)
    results_dst.mkdir(parents=True, exist_ok=True)

    results_dump_file = results_dst / f"{model_name}_results_dump.csv"

    for row in tqdm(dataset_rows):
        id = literal_eval(row["ID"])

        prompt = create_prompt(
            row, prompt_template, use_system_prompt=use_system_prompt
        )

        start_time = time.time()

        number_of_generated_sequences = 1
        response = pipe(
            prompt,
            max_new_tokens=32,
            num_return_sequences=number_of_generated_sequences,
        )

        runner_end_time = time.time() - start_time

        output = response[0]["generated_text"][2]
        response_to_csv(id, output, runner_end_time, results_dump_file)


def model_evaluation_vllm(
    model_name,
    prompt_template,
    dataset_rows,
    engine,
    RESULTS_PATH,
    use_system_prompt=False,
    lora_request=None,
    sampling_params=None,
):
    results_dst = Path(RESULTS_PATH)
    results_dst.mkdir(parents=True, exist_ok=True)

    results_dump_file = results_dst / f"{model_name}_results_dump.csv"

    id_mapping = {
        idx: {
            "ID": i["ID"],
            "prompt": create_prompt(
                i, prompt_template, use_system_prompt=use_system_prompt
            ),
        }
        for idx, i in enumerate(dataset_rows)
    }

    prompts = [x["prompt"] for x in id_mapping.values()]

    processed_prompts = engine.tokenizer.tokenizer.apply_chat_template(
        prompts, tokenize=False, add_generation_template=True
    )

    request_outputs = vllm_helpers.process_requests(
        engine, processed_prompts, sampling_params, lora_request
    )

    for r_output in request_outputs:
        id = id_mapping[int(r_output.request_id)]["ID"]

        output = r_output.outputs[0].text
        # add assistant dict format to the output
        output = {
            "content": output.replace("<|assistant|>", "").strip(),
            "role": "assistant",
        }

        response_to_csv(id, output, 0, results_dump_file)


# Function to generate classification report and other metrics
def get_ground_truth_multi_label(results_dump_file, DATASET_PATH):
    df_response = pd.read_csv(results_dump_file)
    df_dataset = pd.read_csv(DATASET_PATH, usecols=["ID", "ground_truth"])
    merged_df = pd.merge(df_response, df_dataset, on="ID")
    return merged_df


def generate_multi_label_classification_report(
    llm_results_files, output_results_dst, DATASET_PATH
):
    os.makedirs(output_results_dst, exist_ok=True)

    target_labels = [
        "helper_functions",
        "load_data",
        "data_exploration",
        "data_preprocessing",
        "evaluation",
        "modelling",
        "prediction",
        "result_visualization",
        "save_results",
        "comment_only",
    ]

    consolidated_report = {}

    for results_dump_file in llm_results_files:
        llm_name = os.path.basename(results_dump_file).split("_results_dump.csv")[0]
        data_df = get_ground_truth_multi_label(results_dump_file, DATASET_PATH)
        ground_truth_labels = data_df["ground_truth"].apply(lambda x: eval(str(x)))
        predicted_labels = data_df["parsed_response"].apply(lambda x: eval(str(x)))

        mlb = MultiLabelBinarizer(classes=target_labels)
        mlb.fit([[]])
        ground_truth_binary = mlb.transform(ground_truth_labels)
        pred_response_binary = mlb.transform(predicted_labels)

        report = classification_report(
            y_true=ground_truth_binary,
            y_pred=pred_response_binary,
            target_names=target_labels,
            output_dict=True,
        )

        df = pd.DataFrame(report).transpose()
        # df.to_csv(
            # output_results_dst / f"{llm_name}_multi_label_classification_report.csv"
        # )
        df.to_csv(
            output_results_dst / f"{llm_name}_single_label_classification_report.csv"
        )

        subset_accuracy = accuracy_score(ground_truth_binary, pred_response_binary)
        hamming_loss_val = hamming_loss(ground_truth_binary, pred_response_binary)
        jaccard_similarity = jaccard_score(
            ground_truth_binary, pred_response_binary, average="samples"
        )

        consolidated_report[llm_name] = {
            "subset_accuracy": subset_accuracy,
            "hamming_loss_val": hamming_loss_val,
            "jaccard_similarity": jaccard_similarity,
            "precision_micro_avg": report["micro avg"]["precision"],
            "recall_micro_avg": report["micro avg"]["recall"],
            "f1_micro_avg": report["micro avg"]["f1-score"],
            "support_micro_avg": report["micro avg"]["support"],
            "precision_macro_avg": report["macro avg"]["precision"],
            "recall_macro_avg": report["macro avg"]["recall"],
            "f1_macro_avg": report["macro avg"]["f1-score"],
            "support_macro_avg": report["macro avg"]["support"],
            "precision_weighted_avg": report["weighted avg"]["precision"],
            "recall_weighted_avg": report["weighted avg"]["recall"],
            "f1_weighted_avg": report["weighted avg"]["f1-score"],
            "support_weighted_avg": report["weighted avg"]["support"],
            "precision_samples_avg": report["samples avg"]["precision"],
            "recall_samples_avg": report["samples avg"]["recall"],
            "f1_samples_avg": report["samples avg"]["f1-score"],
            "support_samples_avg": report["samples avg"]["support"],
        }

    consolidated_df = pd.DataFrame(consolidated_report).transpose()
    consolidated_df.to_csv(
        output_results_dst / "consolidated_classification_report.csv", mode="a"
    )


def main():
    HF_TOKEN = "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"
    DATASET_PATH = "/home/ssegpu/master-thesis-of-akshita-dubey/workshop/ollama_infer/data_folder/test_data.csv"
    RESULTS_PATH = "/home/ssegpu/.scrapy/results_vllm"
    OUTPUT_RESULTS_PATH = os.path.join(RESULTS_PATH, "Classification_results")
    # Format: (model_name, use_system_prompt)
    # set HF_TOKEN env to the token provided by Hugging Face
    os.environ["HF_TOKEN"] = HF_TOKEN

    models_config = [
        {
            "name": "TinyLlama-1.1B-Chat-v1.0-base-single-label",
            "model_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "quantization": "bitsandbytes",
            "lora_repo": None,
            "use_system_prompt": True,
            "use_vllms_for_evaluation": False,
            "max_model_len": 2048,
        },
        {
            "name": "TinyLlama-1.1B-Chat-v1.0-finetuned-single-label",
            "model_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "quantization": "bitsandbytes",
            "lora_repo": "/home/ssegpu/fine-tuning-apsv/models_single_label/TinyLlama-TinyLlama-1-1B-Chat-v1-0-single-label-v1",
            "use_system_prompt": True,
            "use_vllms_for_evaluation": False,
            "max_model_len": 2048,
        },
        # TODO: issue with Lora
        {
            "name": "mixtral-8-7B-v1-base-single-label",
            "model_path": "mistralai/Mixtral-8x7B-Instruct-v0.1",
            "quantization": "bitsandbytes",
            "lora_repo": None,
            "use_system_prompt": True,
            "use_vllms_for_evaluation": False,
            "max_model_len": 2048,
        },
        {
            "name": "mixtral-8-7B-v1-finetuned-single-label",
            "model_path": "mistralai/Mixtral-8x7B-Instruct-v0.1",
            "quantization": "bitsandbytes",
            "lora_repo": "/home/ssegpu/fine-tuning-apsv/models_single_label/mistralai-Mixtral-8x7B-v0-1-single-label-v1",
            "use_system_prompt": True,
            "use_vllms_for_evaluation": False,
            "max_model_len": 2048,
        },
        {
            "name": "llama3.1-8b-instruct-base-single-label",
            "model_path": "meta-llama/Meta-Llama-3.1-8B-Instruct",
            "quantization": "bitsandbytes",
            "lora_repo": None,
            "use_system_prompt": True,
            "use_vllms_for_evaluation": False,
            "max_model_len": 8192,
        },
        {
            "name": "llama3.1-8b-instruct-finetuned-single-label",
            "model_path": "meta-llama/Meta-Llama-3.1-8B-Instruct",
            "quantization": "bitsandbytes",
            "lora_repo": "/home/ssegpu/fine-tuning-apsv/models_single_label/meta-llama-Meta-Llama-3-1-8B-Instruct-sl-v1",
            "use_system_prompt": True,
            "use_vllms_for_evaluation": False,
            "max_model_len": 8192,
        },
        # # TODO: Cannot use apply_chat_template() because tokenizer.chat_template is not set and no template argument was passed!
        # {
        #     "name": "codellama-70b-python-base",
        #     "model_path": "meta-llama/CodeLlama-70b-Python-hf",
        #     "quantization": "bitsandbytes",
        #     "lora_repo": None,
        #     "use_system_prompt": True,
        #     "use_vllms_for_evaluation": False,
        #     "max_model_len": 4096,
        # },
        # # TODO: Cannot use apply_chat_template() because tokenizer.chat_template is not set and no template argument was passed!
        # {
        #     "name": "codellama-70b-python-finetuned",
        #     "model_path": "meta-llama/CodeLlama-70b-Python-hf",
        #     "quantization": "bitsandbytes",
        #     "lora_repo": "/home/ssegpu/fine-tuning-apsv/models/codellama-70B-python-finetuned-apsv",
        #     "use_system_prompt": True,
        #     "use_vllms_for_evaluation": False,
        #     "max_model_len": 4096,
        # },
        # TODO: Cannot use apply_chat_template() because tokenizer.chat_template is not set and no template argument was passed!
        {
            "name": "codellama-34b-python-base-single-label",
            "model_path": "meta-llama/CodeLlama-34b-Python-hf",
            "quantization": "bitsandbytes",
            "lora_repo": None,
            "use_system_prompt": True,
            "use_vllms_for_evaluation": False,
            "max_model_len": 8192,
        },
        {
            "name": "codellama-34b-python-finetuned-single-label",
            "model_path": "meta-llama/CodeLlama-34b-Python-hf",
            "quantization": "bitsandbytes",
            "lora_repo": "/home/ssegpu/fine-tuning-apsv/models_single_label/codellama-CodeLlama-34b-Python-hf-single-label-v1",
            "use_system_prompt": True,
            "use_vllms_for_evaluation": False,
            "max_model_len": 8192,

        },
        {
            "name": "codellama-13b-python-base-single-label",
            "model_path": "meta-llama/CodeLlama-13b-Python-hf",
            "quantization": "bitsandbytes",
            "lora_repo": None,
            "use_system_prompt": True,
            "use_vllms_for_evaluation": False,
            "max_model_len": 8192,

        },
        {
            "name": "codellama-13b-python-finetuned-single-label",
            "model_path": "meta-llama/CodeLlama-13b-Python-hf",
            "quantization": "bitsandbytes",
            "lora_repo": "/home/ssegpu/fine-tuning-apsv/models_single_label/codellama-CodeLlama-13b-Python-hf-single-label-v1",
            "use_system_prompt": True,
            "use_vllms_for_evaluation": False,
            "max_model_len": 8192,

        },
        {
            "name": "codegemma-2b-base-single-label",
            "model_path": "google/codegemma-2b",
            "quantization": "bitsandbytes",
            "lora_repo": None,
            "use_system_prompt": True,
            "use_vllms_for_evaluation": False,
            "max_model_len": 8192,

        },
        {
            "name": "codegemma-2b-finetuned-single-label",
            "model_path": "google/codegemma-2b",
            "quantization": "bitsandbytes",
            "lora_repo": "/home/ssegpu/fine-tuning-apsv/models_single_label/google-codegemma-2b-sl-v1",
            "use_system_prompt": True,
            "use_vllms_for_evaluation": False,
            "max_model_len": 8192,

        },
    ]

    prompt_template_multi_label = [
        {
            "role": "system",
            "content": "You are an expert in machine learning and your task is to classify the code snippets into categories from the provided list of categories. List of categories: ['helper_functions', 'load_data', 'data_exploration', 'data_preprocessing', 'evaluation', 'modelling', 'prediction', 'result_visualization', 'save_results', 'comment_only']",
        },
        {
            "role": "user",
            "content": "Identify to which category the provided code belongs and assign the code snippet to at least 1 but up to 5 categories based on the probabilities, ignore if the probability of the category is negligible. Strictly provide your response as a list. Avoid providing any additional information, commentary, or personal opinions.\\nPython Code:{code}",
        },
    ]
    prompt_template_llama3_no_sys = [
        {
            "role": "user",
            "content": "You are an expert in machine learning and your task is to classify the code snippets into categories from the provided list of categories. List of categories: ['helper_functions', 'load_data', 'data_exploration', 'data_preprocessing', 'evaluation', 'modelling', 'prediction', 'result_visualization', 'save_results', 'comment_only']. Identify to which category the provided code belongs and assign the code snippet to at least 1 but up to 5 categories based on the probabilities, ignore if the probability of the category is negligible. Strictly provide your response as a list. Avoid providing any additional information, commentary, or personal opinions.\\nPython Code:{code}",
        },
        {"role": "assistant", "content": "{ground_truth}"},
    ]
    prompt_template = [
        {
            "role": "system",
            "content": "You are an expert in machine learning and your task is to classify code snippets into one of the following categories: ['helper_functions', 'load_data', 'data_exploration', 'data_preprocessing', 'evaluation', 'modelling', 'prediction', 'result_visualization', 'save_results', 'comment_only']."
        },
        {
            "role": "user", 
            "content": "Identify the single most appropriate category to which the provided code belongs. Provide your response as a single category label without any additional information, commentary, or personal opinions.\\nPython Code:{code}"
        },
    ]

    llm_results_files = []
    for model in models_config:
        start_time = time.time()
        try:
            dataset_rows = load_dataset(DATASET_PATH)
            if model["use_vllms_for_evaluation"]:
                engine = vllm_helpers.initialize_engine(
                    model["model_path"],
                    model["quantization"],
                    model["lora_repo"],
                    model["max_model_len"],
                )
                lora_request = None
                if model["lora_repo"] is not None:
                    lora_request = LoRARequest(
                        f"{model['name']}-lora", 1, model["lora_repo"]
                    )

                sampling_params = SamplingParams(
                    temperature=0.001, top_p=0.95, max_tokens=32
                )

                model_evaluation_vllm(
                    model["name"],
                    prompt_template,
                    dataset_rows,
                    engine,
                    RESULTS_PATH,
                    use_system_prompt=model["use_system_prompt"],
                    lora_request=lora_request,
                    sampling_params=sampling_params,
                )

                # Clean up the GPU memory for the next test
                del engine
                gc.collect()
                torch.cuda.empty_cache()
            else:
                if model["lora_repo"] is not None:
                    model_path = model["lora_repo"]
                else:
                    model_path = model["model_path"]

                pipe = load_model_and_configurations(HF_TOKEN, model_path)
                model_evaluation(
                    model["name"],
                    (
                        prompt_template
                        if model["use_system_prompt"]
                        else prompt_template_llama3_no_sys
                    ),
                    dataset_rows,
                    pipe,
                    RESULTS_PATH,
                    use_system_prompt=model["use_system_prompt"],
                )

                del pipe
                gc.collect()
                torch.cuda.empty_cache()

            llm_results_files.append(
                os.path.join(RESULTS_PATH, f"{model['name']}_results_dump.csv")
            )
        except Exception as e:
            print(f"Error evaluating model {model['name']}: {e}")

        print(f"Model {model['name']} took {time.time() - start_time} seconds")

    output_results_dst = Path(OUTPUT_RESULTS_PATH)
    output_results_dst.mkdir(parents=True, exist_ok=True)
    generate_multi_label_classification_report(
        llm_results_files, output_results_dst, DATASET_PATH
    )


if __name__ == "__main__":
    main()
