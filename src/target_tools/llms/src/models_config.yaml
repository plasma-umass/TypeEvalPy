models:
  - name: "TinyLlama-1.1B-Chat-v1.0"
    model_path: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    quantization: "bitsandbytes"
    lora_repo: null
    use_system_prompt: true
    use_vllms_for_evaluation: true
    max_model_len: 2048

  - name: "CodeLlama-13b-Instruct"
    model_path: "meta-llama/CodeLlama-13b-Instruct-hf"
    quantization: "bitsandbytes"
    lora_repo: null
    use_system_prompt: true
    use_vllms_for_evaluation: true
    max_model_len: 2048

  - name: "mixtral-8-7B-v1"
    model_path: "mistralai/Mixtral-8x7B-Instruct-v0.1"
    quantization: "bitsandbytes"
    lora_repo: null
    use_system_prompt: true
    use_vllms_for_evaluation: false
    max_model_len: 2048

  - name: "llama3.1-8b-instruct"
    model_path: "meta-llama/Meta-Llama-3.1-8B-Instruct"
    quantization: "bitsandbytes"
    lora_repo: null
    use_system_prompt: true
    use_vllms_for_evaluation: false
    max_model_len: 8192

  - name: "codellama-34b-python"
    model_path: "meta-llama/CodeLlama-34b-Python-hf"
    quantization: "bitsandbytes"
    lora_repo: null
    use_system_prompt: true
    use_vllms_for_evaluation: false
    max_model_len: 8192

  - name: "codellama-13b-python"
    model_path: "meta-llama/CodeLlama-13b-Python-hf"
    quantization: "bitsandbytes"
    lora_repo: null
    use_system_prompt: true
    use_vllms_for_evaluation: false
    max_model_len: 8192

  - name: "codegemma-2b"
    model_path: "google/codegemma-2b"
    quantization: "bitsandbytes"
    lora_repo: null
    use_system_prompt: true
    use_vllms_for_evaluation: false
    max_model_len: 8192

custom_models:
  - name: "TinyLlama-1.1B-Chat-v1.0-finetuned"
    model_path: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    quantization: "bitsandbytes"
    lora_repo: "/home/ssegpu/fine-tuning-apsv/models_single_label/TinyLlama-TinyLlama-1-1B-Chat-v1-0-single-label-v1"
    use_system_prompt: true
    use_vllms_for_evaluation: true
    max_model_len: 2048
  
  - name: "mixtral-8-7B-v1"
    model_path: "mistralai/Mixtral-8x7B-Instruct-v0.1"
    quantization: "bitsandbytes"
    lora_repo: "/home/ssegpu/fine-tuning-apsv/models_single_label/mistralai-Mixtral-8x7B-v0-1-single-label-v1"
    use_system_prompt: true
    use_vllms_for_evaluation: false
    max_model_len: 2048

  - name: "llama3.1-8b-instruct"
    model_path: "meta-llama/Meta-Llama-3.1-8B-Instruct"
    quantization: "bitsandbytes"
    lora_repo: "/home/ssegpu/fine-tuning-apsv/models_single_label/meta-llama-Meta-Llama-3-1-8B-Instruct-sl-v1"
    use_system_prompt: true
    use_vllms_for_evaluation: false
    max_model_len: 8192
  
  - name: "codellama-34b-python"
    model_path: "meta-llama/CodeLlama-34b-Python-hf"
    quantization: "bitsandbytes"
    lora_repo: "/home/ssegpu/fine-tuning-apsv/models_single_label/codellama-CodeLlama-34b-Python-hf-single-label-v1"
    use_system_prompt: true
    use_vllms_for_evaluation: false
    max_model_len: 8192

  - name: "codellama-13b-python"
    model_path: "meta-llama/CodeLlama-13b-Python-hf"
    quantization: "bitsandbytes"
    lora_repo: "/home/ssegpu/fine-tuning-apsv/models_single_label/codellama-CodeLlama-13b-Python-hf-single-label-v1"
    use_system_prompt: true
    use_vllms_for_evaluation: false
    max_model_len: 8192
  
  - name: "codegemma-2b"
    model_path: "google/codegemma-2b"
    quantization: "bitsandbytes"
    lora_repo: "/home/ssegpu/fine-tuning-apsv/models_single_label/google-codegemma-2b-sl-v1"
    use_system_prompt: true
    use_vllms_for_evaluation: false
    max_model_len: 8192  
